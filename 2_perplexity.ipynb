{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Linguistic Complexity of German Abitur Texts from 1963–2013\n",
    "## 2: Perplexity\n",
    "\n",
    "Author: Noemi Kapusta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure:\n",
    "\n",
    "1. Import Libraries and Functions\n",
    "1. Edit the Abitur Texts\n",
    "1. POS-Tagging\n",
    "    - 3.1 Tag the Abitur Texts\n",
    "    - 3.2 Tag the Journals\n",
    "1. Compute Perplexity\n",
    "1. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Functions \n",
    "- libraries: someweta, nltk.tokenize, nltk.data, pandas, numpy\n",
    "- imported function: functions.py\n",
    "    - Function that splits data into test- and devset\n",
    "    - Author: Matilda Schauf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "from someweta import ASPTagger\n",
    "# we use the model \"german_newspaper_2020-05-28.model\"\n",
    "someweta_model = \"/tmp/german_newspaper_2020-05-28.model\"\n",
    "\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "import functions\n",
    "from functions import get_filenames\n",
    "\n",
    "# Get devset data\n",
    "#dev_filenames = get_filenames(\"src/dataSplits.csv\", test=False)\n",
    "dev_filenames = get_filenames(\"src/demo_dataSplits.csv\", test=False)\n",
    "\n",
    "# Get testset data\n",
    "#test_filenames = get_filenames(\"src/dataSplits.csv\", test=True)\n",
    "test_filenames = get_filenames(\"src/demo_dataSplits.csv\", test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Edit the Abitur Texts\n",
    "- read files in and keep only the \"FORM\"-column\n",
    "- save the changed files as txt-files in the folder \"data_tmp/perplex/tokens/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_data(filenames, path_name):\n",
    "    \"\"\"Function that reads in the files and stores only the FORM-column \n",
    "    as txt-files in \"data/tmp_perplex\"\n",
    "    Input: filenames(list): names with the dev- or test-filenames\n",
    "           path_name(str): file path to the data\n",
    "    Output: out_path (str): file path with the folder to store the txt-file\"\"\"\n",
    "    for filename in filenames:\n",
    "        # path to the Data articles\n",
    "        pfad = path_name + filename\n",
    "\n",
    "        # Take the column names from the first line of the file\n",
    "        with open(pfad, \"r\", encoding=\"UTF-8\") as file:\n",
    "            column_names = file.readline().replace(\"# global.columns =\", \"\").strip().split()\n",
    "\n",
    "        # create dataframe \n",
    "        df = pd.read_csv(pfad, comment=\"#\", sep=\"\\t\", quoting=3, header=None, names=column_names)\n",
    "        df.head()\n",
    "        \n",
    "        df = df.astype({\"UEBERSCHRIFT\": str}, errors='raise')\n",
    "        df = df[df.UEBERSCHRIFT == \"0\"]\n",
    "\n",
    "        # keep only the column \"FORM\"\n",
    "        keep = [\"FORM\"]\n",
    "\n",
    "        # save the new Dataframe\n",
    "        df = df[keep]\n",
    "\n",
    "        # save the new Dataframe as a new file in the folder data_tmp/perplex/tokens/\n",
    "        out_path = os.path.curdir + \"/data_tmp/perplex/tokens/\"\n",
    "        os.makedirs(out_path, exist_ok=True)  \n",
    "\n",
    "        path_out = out_path + filename\n",
    "        with open(path_out, mode=\"w\", encoding='UTF-8') as outfile:\n",
    "            dfAsString = df.to_string(header=False, index=False)\n",
    "            outfile.write(dfAsString)\n",
    "\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = \"data/\"\n",
    "path_abitexts = save_data(test_filenames, path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. POS-Tagging\n",
    "- tag the files with the Someweta POS-Tagger\n",
    "- we run the POS tagger with the model created by SoMeWeTa from German newspapers called \"german_newspaper_2020-05-28\" \n",
    "- Learn more to this tagger under the following link: https://github.com/tsproisl/SoMeWeTa\n",
    "\n",
    "### 3.1 Tag the Abitur Texts\n",
    "- tag the Abitur Text files with their POS\n",
    "- the POS-Tags of a sentence are stored in one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "someweta_model = \"/tmp/german_newspaper_2020-05-28.model\"\n",
    "\n",
    "def tag_abitexts(file):\n",
    "    \"\"\"Function that reads files of the file path of the abitexts and saves their tags.\n",
    "    Input: file (str) = file path of the files to be read in.\n",
    "    Output: out_path (str) = path to the folder where the tags are saved\"\"\"\n",
    "\n",
    "    # Run the POS tagger with the model created by SoMeWeTa from German newspapers\n",
    "    #model = \"german_newspaper_2020-05-28.model\"\n",
    "    newstagger = ASPTagger()\n",
    "    newstagger.load(someweta_model)\n",
    "\n",
    "    # Open and read in articles from the list\n",
    "    for file_name in os.listdir(file):\n",
    "        infile = open(file + file_name, mode=\"r\", encoding='UTF8')\n",
    "        corpusliste = infile.readlines()\n",
    "        infile.close()\n",
    "        \n",
    "        corpustext = str()\n",
    "\n",
    "        # delete or change the unused tokens\n",
    "        for element in corpusliste:\n",
    "            element = element.replace(\"\\n\", \"\").strip(\" \")\n",
    "            if element[:4] == \"<B->\":\n",
    "                corpustext += element[4:] + \" \"\n",
    "            elif element[:4] == \"<E->\":\n",
    "                continue\n",
    "            elif element[:7] == \"<EMPTY>\":\n",
    "                continue\n",
    "            elif element[:4] == \"<I->\":\n",
    "                continue\n",
    "            else:\n",
    "                corpustext += element + \" \"\n",
    "                \n",
    "        # tokenize corpustext in sentences\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "        tok_text = tokenizer.tokenize(corpustext)\n",
    "\n",
    "        newstagged_liste = list()\n",
    "\n",
    "        # tokenize the sentences in tokens and tag these with our trained POS-tagger\n",
    "        for sent in tok_text:\n",
    "            toks = word_tokenize(sent)\n",
    "            newstagged_sentence = newstagger.tag_sentence(toks)\n",
    "            newstagged_liste.append(newstagged_sentence)\n",
    "\n",
    "        pos_tags = str()\n",
    "\n",
    "        # one line per sentence and only store the tags in a new document\n",
    "        for sent in newstagged_liste:\n",
    "            for tok in sent:\n",
    "                pos_tags += tok[1] + \" \"\n",
    "            pos_tags += \"\\n\"\n",
    "\n",
    "        out_path = os.path.curdir + \"/data_tmp/perplex/tagged/\"\n",
    "        os.makedirs(out_path, exist_ok=True)  \n",
    "\n",
    "        path_out = out_path + file_name\n",
    "        with open(path_out, mode=\"w\", encoding='UTF-8') as outfile:\n",
    "            outfile.write(pos_tags)\n",
    "\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abitexts_tagged = tag_abitexts(path_abitexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tag the newspaper texts\n",
    "\n",
    "- tag the newspaper files (ZEIT, Express) with their POS\n",
    "- the POS-Tags of a sentence are stored in one line\n",
    "- save results in separate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "someweta_model = \"/tmp/german_newspaper_2020-05-28.model\"\n",
    "\n",
    "def tag_articles(file, filelist, name):\n",
    "    \"\"\"Function that reads files of a file path and saves their tags.\n",
    "    Input:  file (str) = file path of the files to be read in.\n",
    "            filelist (list) = list with the filenames\n",
    "            name(str) = name of the folder where the POS-Tags of the articles are to be saved\n",
    "    Output: corpustext (str) = read files together as string\"\"\"\n",
    "\n",
    "    # Run the POS tagger with the model created by SoMeWeTa from German newspapers\n",
    "    # Learn more to this tagger under the following link: https://github.com/tsproisl/SoMeWeTa\n",
    "    newstagger = ASPTagger()\n",
    "    newstagger.load(someweta_model)\n",
    "\n",
    "    # Open and read in articles from the list\n",
    "    for file_name in os.listdir(file):\n",
    "        if file_name in filelist:\n",
    "            infile = open(file + file_name, mode=\"r\", encoding='UTF8')\n",
    "            corpustext = infile.read()\n",
    "            infile.close()\n",
    "\n",
    "            # tokenize corpustext in sentences\n",
    "            tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "            tok_text = tokenizer.tokenize(corpustext)\n",
    "\n",
    "            newstagged_liste = list()\n",
    "\n",
    "            # tokenize the sentences in tokens and tag these with our trained POS-tagger\n",
    "            for sent in tok_text:\n",
    "                toks = word_tokenize(sent)\n",
    "                newstagged_sentence = newstagger.tag_sentence(toks)\n",
    "                newstagged_liste.append(newstagged_sentence)\n",
    "\n",
    "            pos_tags = str()\n",
    "\n",
    "            # one line per sentence and only store the tags in a new document\n",
    "            for sent in newstagged_liste:\n",
    "                for tok in sent:\n",
    "                    pos_tags += tok[1] + \" \"\n",
    "                pos_tags += \"\\n\"\n",
    "\n",
    "            out_path = os.path.curdir + \"/\" + foldername\n",
    "            os.makedirs(out_path, exist_ok=True)  \n",
    "\n",
    "            path_out = out_path + file_name\n",
    "            with open(path_out, mode=\"w\", encoding='UTF-8') as outfile:\n",
    "                outfile.write(pos_tags)\n",
    "            \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ZEIT articles\n",
    "data_path = \"data/\"\n",
    "\n",
    "files = [\"zeit_1.spl\", \"zeit_2.spl\", \"zeit_3.spl\"]\n",
    "foldername = \"data_tmp/perplex/tagged/zeit/\"\n",
    "os.makedirs(foldername, exist_ok=True)  \n",
    "zeit_tagged = tag_articles(data_path, files, foldername)\n",
    "\n",
    "#for Express articles\n",
    "foldername = \"data_tmp/perplex/tagged/express/\"\n",
    "files = [\"express_1.spl\", \"express_2.spl\", \"express_3.spl\"]\n",
    "os.makedirs(foldername, exist_ok=True)  \n",
    "express_tagged = tag_articles(data_path, files, foldername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Perplexity\n",
    "- Compute perplexity according to Jurafsky and Martin (2022)\n",
    "- Training:\n",
    "    - Train one model with the ZEIT articles\n",
    "    - Train another model with the Express articles\n",
    "- Testing:\n",
    "    - Use the Abitur texts per year\n",
    "    - Use 5000 Bigrams divided evenly among all texts per year\n",
    "    - Compute perplexity for each text, and then the mean and the standard deviation per year\n",
    "- The lower the perplexity value, the better the model predicts the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(traincorpus):\n",
    "    \"\"\"Function that creates a list with the padded tokens\n",
    "    Input: traincorpus(path): file path to the corpus with the data for the training\n",
    "    Ouput: corpuslist (list): list with the padded tokens\"\"\"\n",
    "    \n",
    "    corplist = list()\n",
    "    for file_name in os.listdir(traincorpus):\n",
    "        infile = open(traincorpus + file_name, mode=\"r\", encoding='UTF8')\n",
    "        corplist += infile.readlines()\n",
    "        infile.close()\n",
    "\n",
    "    # Padding\n",
    "    corpustext = \"\"\n",
    "\n",
    "    # erase Newline and add <s> and </s> at the beginning and ending of a sentence\n",
    "    for sentence in corplist:\n",
    "        sentence = sentence.strip()\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "        paddedsentence = \"<s> \" + sentence + \" </s> \"\n",
    "        corpustext = corpustext + paddedsentence\n",
    "\n",
    "    corpuslist = corpustext.split()\n",
    "\n",
    "    return corpuslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(textlist):\n",
    "    \"\"\"Function that counts the number of words\n",
    "    Input: textlist (list) = List with tokens whose words are to be counted\n",
    "    Output: numberN (int) = Number of words N of a text\"\"\"\n",
    "\n",
    "    numberN = 0\n",
    "\n",
    "    # count all tokens except \"<s>\"\n",
    "    for word in textlist:\n",
    "        if word == \"<s>\":\n",
    "            continue\n",
    "        else:\n",
    "            numberN += 1\n",
    "\n",
    "    return numberN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_types(textlist):\n",
    "    \"\"\"Function that counts the number of word types\n",
    "    Input: textlist (list) = List with tokens whose word types are to be counted\n",
    "    Output: freq_wordtypes (dict) = Dictionary with the types and their frequencies\"\"\"\n",
    "\n",
    "    # initialize a dictionary: key = word type; value = frequency\n",
    "    freq_wordtypes = dict()\n",
    "\n",
    "    # count the frequencies of the word types and save them in a dictionary\n",
    "    for word in textlist:\n",
    "        freq_wordtypes[word] = freq_wordtypes.get(word, 0) + 1\n",
    "\n",
    "    return freq_wordtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigrams(textlist):\n",
    "    \"\"\"Function that creates bigrams from a text and counts the frequencies of the bigram types\n",
    "    Input: textlist(list) = List with tokens to create the bigrams\n",
    "    Output: freqs(dict) = Dictionary with the bigram types and their frequencies\"\"\"\n",
    "\n",
    "    # initialize a dictionary: key = bigramm; value = frequency\n",
    "    freqs = dict()\n",
    "\n",
    "    # get all the bigram types with their frequencies, with the exception of the bigram (\"<s>\", \"</s>\")\n",
    "    for index in range(len(textlist) - 1):\n",
    "        bigram = (textlist[index],) + (textlist[index + 1],)\n",
    "        if not bigram == (\"</s>\", \"<s>\"):\n",
    "            freqs[bigram] = freqs.get(bigram, 0) + 1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(testbigr, trainbigr, trainunigr, numberwords):\n",
    "    \"\"\"Function that calculates the probabiblity and the perplexity of a model to a test set\n",
    "    input: testbigr(dict): dictionary with bigram types + frequencies from the test set\n",
    "            trainbigr(dict): Dictionary with bigram types + frequencies from the training set\n",
    "            trainunigr(dict): Dictionary with unigram types + frequencies from training set\n",
    "            numberwords(int): number of words of the test set\n",
    "    output: perplexity: Perplexity of a model\"\"\"\n",
    "    \n",
    "    prob = 0\n",
    "    \n",
    "    # calculate log probability \n",
    "    for bigr in testbigr:\n",
    "        bigr_value = testbigr[bigr]\n",
    "        if bigr in trainbigr:\n",
    "            trainbigr_value = trainbigr[bigr]\n",
    "            unigr_value = trainunigr[bigr[0]]\n",
    "            bigr_prob = numpy.log(trainbigr_value/unigr_value)\n",
    "            prob += (bigr_prob * bigr_value)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # perplexity is calculated by taking the probability to the power of -1 \n",
    "    # divided by the number of words in the test set.\n",
    "    perpl = numpy.exp(-prob/numberwords)\n",
    "\n",
    "    return perpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "\n",
    "# Model 1 ZEIT:\n",
    "# read text and padding\n",
    "model1 = create_model(zeit_tagged)\n",
    "# count words of the first model:\n",
    "model1_words = count_words(model1)\n",
    "# count wordtypes of the first model\n",
    "m1_wordtypes = count_types(model1)\n",
    "# count bigramtypes of the first model\n",
    "m1_bigram_types = create_bigrams(model1)\n",
    "\n",
    "# Model 2 Express:\n",
    "# read text and padding\n",
    "model2 = create_model(express_tagged)\n",
    "# count words of the second model\n",
    "model2_words = count_words(model2)\n",
    "# count wordtypes of the second model\n",
    "m2_wordtypes = count_types(model2)\n",
    "# count bigramtypes of the second model\n",
    "m2_bigram_types = create_bigrams(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigr_n: 2500\n",
      "articlenumber: 2\n",
      "bigr_n: 2500\n",
      "articlenumber: 2\n"
     ]
    }
   ],
   "source": [
    "# Testing with tagged abitexts\n",
    "\n",
    "# Testset per year:\n",
    "#yearlist = [\"1963\", \"1968\", \"1974\", \"1978\", \"1983\", \"1988\", \"1993\", \"1998\", \"2003\", \"2008\", \"2013\"]\n",
    "yearlist = [\"1963\", \"2013\"]\n",
    "allppl_zeit_list = list()\n",
    "allppl_expr_list = list()\n",
    "meanppl_zeit = list()\n",
    "meanppl_expr = list()\n",
    "stdppl_zeit = list ()\n",
    "stdppl_expr = list()\n",
    "\n",
    "\n",
    "for i in range(len(yearlist)):\n",
    "    #initialize article_counter per year and 2 lists for zeit and express perplexity\n",
    "    artcounter = 0\n",
    "    ppl_zeit_list = list()\n",
    "    ppl_expr_list = list()\n",
    "    \n",
    "    #calculate how many bigrams per text must be used as a test set, so that 5000 bigrams are used per year\n",
    "    for file_name in os.listdir(abitexts_tagged):\n",
    "        if yearlist[i] in file_name:\n",
    "            artcounter +=1\n",
    "    bigr_n = 5000/artcounter\n",
    "    bigr_n = round(bigr_n)\n",
    "    print(\"bigr_n:\", bigr_n)\n",
    "    print(\"articlenumber:\", artcounter)\n",
    "\n",
    "    for file_name in os.listdir(abitexts_tagged):\n",
    "        if yearlist[i] in file_name:\n",
    "            infile = open(abitexts_tagged + file_name, mode=\"r\", encoding='UTF8')\n",
    "            corplist = infile.readlines()\n",
    "            infile.close()\n",
    "            \n",
    "            # Padding\n",
    "            corpustext = \"\"    \n",
    "            # erase Newline and add <s> and </s> at the beginning and ending of a sentence\n",
    "            for sentence in corplist:\n",
    "                sentence = sentence.strip()\n",
    "                if len(sentence) == 0:\n",
    "                    continue\n",
    "                paddedsentence = \"<s> \" + sentence + \" </s> \"\n",
    "                corpustext = corpustext + paddedsentence\n",
    "            corpuslist = corpustext.split()\n",
    "            \n",
    "            # count words\n",
    "            test_words = count_words(corpuslist)\n",
    "            # create bigrams\n",
    "            test_bigrams = create_bigrams(corpuslist)\n",
    "            \n",
    "            testbigr = {}\n",
    "            \n",
    "            try:\n",
    "                for bigr in itertools.islice(test_bigrams, 0, bigr_n):\n",
    "                    testbigr[bigr] = test_bigrams[bigr]\n",
    "            except:\n",
    "                print(\"nicht genügend bigrams:\", len(test_bigrams))\n",
    "                \n",
    "             # Perplexity for model 1\n",
    "            model1_ppl = perplexity(testbigr, m1_bigram_types, m1_wordtypes, test_words)\n",
    "            ppl_zeit_list.append(model1_ppl)\n",
    "\n",
    "            # Perplexity for model 2\n",
    "            model2_ppl = perplexity(testbigr, m2_bigram_types, m2_wordtypes, test_words)\n",
    "            ppl_expr_list.append(model2_ppl)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    allppl_zeit_list.append(ppl_zeit_list)\n",
    "    allppl_expr_list.append(ppl_expr_list)\n",
    "    meanppl_zeit.append(numpy.mean(ppl_zeit_list))\n",
    "    meanppl_expr.append(numpy.mean(ppl_expr_list))\n",
    "    stdppl_zeit.append(numpy.std(ppl_zeit_list))\n",
    "    stdppl_expr.append(numpy.std(ppl_expr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>all_ppl_ZEIT</th>\n",
       "      <th>all_ppl_Express</th>\n",
       "      <th>mean_ppl_ZEIT</th>\n",
       "      <th>mean_ppl_Express</th>\n",
       "      <th>std_ppl_ZEIT</th>\n",
       "      <th>std_ppl_Express</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1963</td>\n",
       "      <td>[5.933789693637949, 5.523221692800902]</td>\n",
       "      <td>[6.199482361595224, 5.558098097603365]</td>\n",
       "      <td>5.728506</td>\n",
       "      <td>5.878790</td>\n",
       "      <td>0.205284</td>\n",
       "      <td>0.320692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>[5.957410118790915, 5.806513552218744]</td>\n",
       "      <td>[6.063355038057216, 5.676248957629498]</td>\n",
       "      <td>5.881962</td>\n",
       "      <td>5.869802</td>\n",
       "      <td>0.075448</td>\n",
       "      <td>0.193553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR                            all_ppl_ZEIT  \\\n",
       "0  1963  [5.933789693637949, 5.523221692800902]   \n",
       "1  2013  [5.957410118790915, 5.806513552218744]   \n",
       "\n",
       "                          all_ppl_Express  mean_ppl_ZEIT  mean_ppl_Express  \\\n",
       "0  [6.199482361595224, 5.558098097603365]       5.728506          5.878790   \n",
       "1  [6.063355038057216, 5.676248957629498]       5.881962          5.869802   \n",
       "\n",
       "   std_ppl_ZEIT  std_ppl_Express  \n",
       "0      0.205284         0.320692  \n",
       "1      0.075448         0.193553  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create dataframes \n",
    "# dictionary for every category\n",
    "ppl = {'YEAR': yearlist, \n",
    "       'all_ppl_ZEIT': allppl_zeit_list, \n",
    "       'all_ppl_Express': allppl_expr_list, \n",
    "       'mean_ppl_ZEIT': meanppl_zeit, \n",
    "       'mean_ppl_Express': meanppl_expr, \n",
    "       'std_ppl_ZEIT': stdppl_zeit, \n",
    "       'std_ppl_Express': stdppl_expr}\n",
    "df_ppl = pd.DataFrame(ppl)\n",
    "pd.set_option(\"max_colwidth\", 500)\n",
    "display(df_ppl)\n",
    "\n",
    "# dataframe to csv-file\n",
    "#out_path = \"results/2_perplex/dev_results\"\n",
    "#out_path = \"results/2_perplex/test_results\"\n",
    "out_path = \"results/2_perplex_demo/\"\n",
    "import os\n",
    "os.makedirs(out_path, exist_ok=True)  \n",
    "\n",
    "df_ppl.to_csv(out_path + \"perplexity.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. References\n",
    "\n",
    "- Daniel Jurafsky and James H. Martin. 2022. Speech and Language Processing.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
