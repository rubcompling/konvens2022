{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Linguistic Complexity of German Abitur Texts from 1963â€“2013\n",
    "## 3: Syntactic Complexity\n",
    "\n",
    "Author: Matilda Schauf\n",
    "\n",
    "This notebook is about analyzing syntactic complexity of German Abitur texts from 1963-2013 using the GraphVar corpus (Berg et al., 2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Import Modules](#import)\n",
    "* [Get Data](#getdata)\n",
    "* [Syntactic Complexity Features](#sycompl)\n",
    "* [Load Class for Measuring Syntactic Complexity](#loadclass)\n",
    "* [Get Results and Analyze Data](#getresults)\n",
    "* [Syntactic Complexity of Express and Zeit](#expresszeit)\n",
    "* [References](#ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules <a class=\"anchor\" id=\"import\"></a>\n",
    "\n",
    "I will use `Pandas` and `NumPy` as well as two functions and a class from two other modules made by me: \n",
    "\n",
    "`get_filenames` and `make_df_dict` from the module `functions` and `SynComplMeas` from the module `calc_syn_complexity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Insert the path of modules folder \n",
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "from functions import get_filenames, make_df_dict\n",
    "from calc_syn_complexity import SynComplMeas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data <a class=\"anchor\" id=\"getdata\"></a>\n",
    "Get filenames for development or test data. For that, use imported function `get_filenames`.\n",
    "\n",
    "Then, read them into data frames and save them in a dictionary. For that, use imported function `make_df_dict`.\n",
    "\n",
    "For more information about the functions, print the docstrings in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function that takes .csv-File with the filenames that have been categorized into dev or test files and returns either the filenames for the development data or the filenames for the test data.\n",
      "        \n",
      "        Input:\n",
      "            1. filenames_categorized (str): Filename of the file with the filenames and their categories\n",
      "            2. test (bool): True when test filenames should be returned, False when development filenames should be returned\n",
      "        Output:\n",
      "            1. filenames (list): List of the wanted conllup filenames (str)\n"
     ]
    }
   ],
   "source": [
    "print(get_filenames.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Function that takes a list of filenames, loads the conllup files into a data frame, and saves them in a dictionary with the key tuples (year, text number).\n",
      "\n",
      "        Input:\n",
      "            1. path (str): Path to the files on the computer\n",
      "            2. filenames (list): List with the filenames of the conllup files (str)\n",
      "        Output:\n",
      "            1. dfs_dict (dict): Dictionary with key = tuple (year, text number) and value = data frame\n"
     ]
    }
   ],
   "source": [
    "print(make_df_dict.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames (comment out one line)\n",
    "\n",
    "# development data\n",
    "#filenames = get_filenames(\"dataSplits.csv\", test=False)\n",
    "\n",
    "# test data\n",
    "#filenames = get_filenames(\"dataSplits.csv\", test=True)\n",
    "\n",
    "# demo data\n",
    "filenames = get_filenames(\"src/demo_dataSplits.csv\", test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original GraphVar data\n",
    "#path = \"data/conll/graphvar_1963-2013_DE_conll/\"\n",
    "\n",
    "# demo data\n",
    "path = \"data/\"\n",
    "df_dict = make_df_dict(path, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Complexity Features <a class=\"anchor\" id=\"syncompl\"></a>\n",
    "This section introduces the syntactic complextiy features that we will use for our analysis with their definitions and notes on literature. \n",
    "\n",
    "The variable names that are dislayed as code are also the names of the attributes of the class that will be loaded in a later cell.\n",
    "\n",
    "- `sent_lens` = **Mean Sentence Length**\n",
    "    - *Mean Sentence Length in Tokens* (Meyer et al., 2020); *Mean length of sentences* (Chen & Zechner, 2011)\n",
    "        - \\# Tokens / # Sentences\n",
    "- `clauses_s` = **Clauses per Sentence**\n",
    "    - *Sentence Coordination Ratio/Sentence Complexity Ratio/T-Unit Complexity Ratio* (Meyer et al., 2020); *number of clauses per sentence* (Chen & Zechner, 2011)\n",
    "        - \\# Clause (paratactic constructions, relative, simplex) tokens / # Sentence\n",
    "- `subc_s` = **Subordinate Clauses per Sentence**\n",
    "    - *Dependent Clauses per T-Unit* (Meyer et al., 2020)\n",
    "        - \\# C / # Sentences\n",
    "- `clause_lens` = **Mean Clause Length** in Tokens\n",
    "    - *Mean Length of Clause* (Meyer et al., 2020); *mean length of clauses* (Chen & Zechner, 2011)\n",
    "        - \\# Clause tokens / # Clauses\n",
    "\n",
    "- `simpx_lens` = **Mean Simplex Clause Length** in Tokens\n",
    "    - *Mean length of simple sentences* (Chen & Zechner, 2011)\n",
    "        - \\# SIMPX tokens / # SIMPX\n",
    "- `relc_lens` = **Mean Relative Clause Length in Tokens** in Tokens\n",
    "    - not listed\n",
    "        - \\# R-SIMPX tokens / # R-SIMPX\n",
    "\n",
    "- `simpx_c` = **Simplex Clause Ratio**\n",
    "    - not listed\n",
    "        - \\# SIMPX / # Clauses\n",
    "- `relc_c` = **Relative Clause Ratio**\n",
    "    - not listed, but mentioned in paper (Meyer et al., 2020)\n",
    "        - \\# R-SIMPX / # Clauses\n",
    "- `parac_c` = **Paratactic Clause Construction Ratio**\n",
    "    - not listed, but mentioned in paper (Meyer et al., 2020)\n",
    "        - \\# P-SIMPX / # Clauses\n",
    "\n",
    "- `vf_lens`, `mf_lens`, `nf_lens` = **Mean Prefield Length**, **Mean Middle Field Length**, **Mean Postfield Lenght** in Node Tags or Tokens\n",
    "    - not listed\n",
    "        - \\# {VF | MF | NF} tokens / # {VF | MF | N}\n",
    "- `nx_lens`, `px_lens` = **Mean Noun Phrase Length**, **Mean Prepositional Phrase Length** in Node Tags\n",
    "    - not listed\n",
    "        - \\# {NX | PX} tokens / # {NX | PX }\n",
    "\n",
    "- `verbx_s` = **Verb Phrases per Sentence**\n",
    "    - *Verb Phrases per T-Unit* (Meyer et al., 2020); *mean number of verbs per sentence* (Chen & Zechner, 2011)\n",
    "        - \\# VXFIN+VXINF tokens / # Sentence\n",
    "- `nx_s` = **Noun Phrases per Sentence**\n",
    "    - *Mean number of noun phrases (NP) per sentence* (Chen & Zechner, 2011)\n",
    "        - \\# NX tokens / # Sentence\n",
    "\n",
    "- `tok_embeds` = **Mean Token Embedding Depth** in Node Tags\n",
    "    - not listed\n",
    "        - \\# Nodes / # Tokens\n",
    "- `max_sent_embeds` = **Mean Maximum Embedding Depth per Sentence** in Node Tags\n",
    "    - not listed\n",
    "        - SUM of maximum embedding depths per sentence / # Sentences\n",
    "\n",
    "- `vv_nn` = **Verb/Noun Ratio**\n",
    "    - not listed\n",
    "        - \\# Verbs (XPOS starts with 'VV') / # nouns (XPOS is 'NN')\n",
    "\n",
    "It was decided to leave out the other features during the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Class for Measuring Syntactic Complexity <a class=\"anchor\" id=\"loadclass\"></a>\n",
    "\n",
    "The cell below loads the class `SynComplMeas` which calculates the syntactic complexity features introduced above. \n",
    "\n",
    "The loading time is about 30 seconds for the development data and about 2 minutes for the test data. \n",
    "\n",
    "For seeing all attributes and methods of the class, print the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load class\n",
    "sc = SynComplMeas(name=\"Syntactical Complexity Measures\", df_dict=df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A class to represent our syntactic complexity measures.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    name : str\n",
      "        Name for the class\n",
      "    df_dict : dict\n",
      "        Dictionary that contains data frames with corpus annotation data for several connlup files\n",
      "    sent_lens : Pandas.DataFrame\n",
      "        Results for feature \"Mean Sentence Length in Tokens\"\n",
      "    tok_embeds : Pandas.DataFrame\n",
      "        Results for feature \"Mean Token Embedding Depth\"\n",
      "    max_sent_embeds : Pandas.DataFrame\n",
      "        Results for feature \"Mean Maximum Embedding Depth per Sentence\"\n",
      "    simpx_s : Pandas.DataFrame\n",
      "        Results for feature \"Simplex Clauses per Sentence\"\n",
      "    subc_s : Pandas.DataFrame\n",
      "        Results for feature \"Dependent Clauses per Sentence\"\n",
      "    relc_s : Pandas.DataFrame\n",
      "        Results for feature \"Relative Clauses per Sentence\"\n",
      "    parac_s : Pandas.DataFrame\n",
      "        Results for feature \"Paratactic Clause Constructions per Sentence\"\n",
      "    clauses_s : Pandas.DataFrame\n",
      "        Results for feature \"Clauses per Sentence\"\n",
      "    verbx_s : Pandas.DataFrame\n",
      "        Results for feature \"Verb Phrases per Sentence\"\n",
      "    vc_s : Pandas.DataFrame\n",
      "        Results for feature \"Verb Complexes per Sentence\"\n",
      "    nx_s : Pandas.DataFrame\n",
      "        Results for feature \"Noun Phrases per Sentece\"\n",
      "    simpx_c : Pandas.DataFrame\n",
      "        Results for feature \"Simplex Clause Ratio\"\n",
      "    subc_c : Pandas.DataFrame\n",
      "        Results for feature \"Dependent Clause Ratio\"\n",
      "    relc_c : Pandas.DataFrame\n",
      "        Results for feature \"Relative Clause Ratio\"\n",
      "    parac_c : Pandas.DataFrame\n",
      "        Results for feature \"Paratactic Clause Construction Ratio\"\n",
      "    clause_lens : Pandas.DataFrame\n",
      "        Results for feature \"Mean Clause Length in Tokens\"\n",
      "    simpx_lens : Pandas.DataFrame\n",
      "        Results for feature \"Mean Simplex Clause Length in Tokens\"\n",
      "    relc_lens : Pandas.DataFrame\n",
      "        Results for feature \"Mean Relative Clause Length in Tokens\"\n",
      "    nx_lens : Pandas.DataFrame\n",
      "        Results for feature \"Mean Noun Phrase Length\"\n",
      "    px_lens : Pandas.DataFrame\n",
      "        Results for feature \"Mean Prepositional Phrase Length\"\n",
      "    vf_lens : Pandas.DataFrame\n",
      "        Results for feature \"Mean Initial Field (Vorfeld) Length\"\n",
      "    mf_lens : Pandas.DataFrame\n",
      "        Results for feature \"Mean Middle Field (Mittelfeld) Length\"\n",
      "    nf_lens : Pandas.DataFrame\n",
      "        Results for feature \"Mean Final Field (Nachfeld) Lenght\"\n",
      "    vv_nn : Pandas.DataFrama\n",
      "        Results for feature \"Verb/Noun Ratio\"\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "    count_pattern(df: Pandas.DataFrame, regex: str):\n",
      "        Counts pattern in the syntax column of a text.\n",
      "    get_tok_embeds(df: Pandas.DataFrame, replace_RE: str):\n",
      "        Calculates feature \"Mean Token Embedding Depth\" for a text.\n",
      "    get_max_embeds(df: Pandas.DataFrame, replace_RE: str, sent_count: int):\n",
      "        Calculates feature \"Mean Maximum Embedding Depth per Sentence\" for a text.\n",
      "    get_clause_lens(df: Pandas.DataFrame, regex: str):\n",
      "        Takes regular expression of a clause's node label and calculates the mean clause length in a text.\n",
      "    get_phrase_lens(df: Pandas.DataFrame, regex: str):\n",
      "        Takes regular expression of a phrase's node label and calculates the mean phrase length in a text.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# show docstring\n",
    "print(sc.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Results and Analyze Data <a class=\"anchor\" id=\"getresults\"></a>\n",
    "\n",
    "The cell below saves the results as `.csv` files in a target directory. \n",
    "\n",
    "You can get the result data frames by using the attribute/feature variables: `sc.attribute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target directory\n",
    "#target_dir = \"results/3_syntax/dev_results/\"\n",
    "#target_dir = \"results/3_syntax/test_results/\"\n",
    "target_dir = \"results/3_syntax_demo/\"\n",
    "\n",
    "import os\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# save result data frames in list\n",
    "result_dfs = [sc.sent_lens, sc.tok_embeds, sc.max_sent_embeds, sc.simpx_s, sc.subc_s, sc.relc_s, sc.parac_s, sc.clauses_s, sc.verbx_s, sc.vc_s, sc.nx_s, sc.simpx_c,\n",
    "        sc.subc_c, sc.relc_c, sc.parac_c, sc.clause_lens, sc.simpx_lens, sc.relc_lens, sc.nx_lens, sc.px_lens, sc.vf_lens, sc.mf_lens, sc.nf_lens, sc.vv_nn]\n",
    "\n",
    "# iterate over data frames + data frame names (str) and save results in .csv files\n",
    "for df, df_name in zip(result_dfs, sc.feature_names):\n",
    "    df.to_csv(target_dir + df_name + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell can be used for displaying a particular result data frame (`sc.attribute`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>YEAR_VAL</th>\n",
       "      <th>STUDENT_VALS</th>\n",
       "      <th>STUDENT_STD</th>\n",
       "      <th>YEARS_MEAN</th>\n",
       "      <th>YEARS_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1963</td>\n",
       "      <td>0.583824</td>\n",
       "      <td>[0.6617647058823529, 0.5058823529411764]</td>\n",
       "      <td>0.110225</td>\n",
       "      <td>0.553828</td>\n",
       "      <td>0.04242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>0.523832</td>\n",
       "      <td>[0.48375451263537905, 0.5639097744360902]</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>0.553828</td>\n",
       "      <td>0.04242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  YEAR_VAL                               STUDENT_VALS  STUDENT_STD  \\\n",
       "0  1963  0.583824   [0.6617647058823529, 0.5058823529411764]     0.110225   \n",
       "1  2013  0.523832  [0.48375451263537905, 0.5639097744360902]     0.056678   \n",
       "\n",
       "   YEARS_MEAN  YEARS_STD  \n",
       "0    0.553828    0.04242  \n",
       "1    0.553828    0.04242  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show result df in this notebook\n",
    "sc.vv_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell gives an overview of what years most often had the **maximum** or **minimum** values for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013\n",
      "1963\n"
     ]
    }
   ],
   "source": [
    "# import function mode from module statistics\n",
    "from statistics import mode\n",
    "\n",
    "# only calculate for features where higher value implies higher complexity\n",
    "dfs = [sc.sent_lens, sc.tok_embeds, sc.max_sent_embeds, sc.subc_s, sc.relc_s, sc.clauses_s, sc.verbx_s, sc.vc_s, sc.nx_s, sc.subc_c, sc.relc_c, sc.clause_lens, sc.simpx_lens, \n",
    "sc.relc_lens, sc.nx_lens, sc.px_lens, sc.vf_lens, sc.mf_lens, sc.nf_lens, sc.vv_nn]\n",
    "\n",
    "# make list of years with the highest values for each feature\n",
    "highest = [int(df.YEAR[df.YEAR_VAL == df.YEAR_VAL.max()]) for df in dfs]\n",
    "# make list of years with the lowest values for each features\n",
    "lowest = [int(df.YEAR[df.YEAR_VAL == df.YEAR_VAL.min()]) for df in dfs]\n",
    "\n",
    "# print years that most often had highest/lowest value\n",
    "print(mode(highest))\n",
    "print(mode(lowest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell prints the *mean student standard deviation* for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4794747333435074 sent_lens\n",
      "0.1956032342631167 tok_embeds\n",
      "0.19117579955012226 max_sent_embeds\n",
      "0.4807085573855916 simpx_s\n",
      "0.20158745954879642 subc_s\n",
      "0.11085095028375574 relc_s\n",
      "0.0449695102070392 parac_s\n",
      "0.46001815219673714 clauses_s\n",
      "0.4497837119427211 verbx_s\n",
      "0.28860235417601315 vc_s\n",
      "0.6539408577890626 nx_s\n",
      "0.07152098826130787 simpx_c\n",
      "0.02358680711941884 subc_c\n",
      "0.050698651298429875 relc_c\n",
      "0.020822336962878024 parac_c\n",
      "0.569159569183477 clause_lens\n",
      "0.4563037538765561 simpx_lens\n",
      "1.8528816581091945 relc_lens\n",
      "0.17795069938646124 nx_lens\n",
      "0.22932140130699966 px_lens\n",
      "0.486404509501187 vf_lens\n",
      "0.4915980588724505 mf_lens\n",
      "0.9733152736626002 nf_lens\n",
      "0.08345189899954328 vv_nn\n"
     ]
    }
   ],
   "source": [
    "for df, name in zip(result_dfs, sc.feature_names):\n",
    "    print(df.STUDENT_STD.mean(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells are for checking (for a result data frame) which rows have values that are higher than the mean for the columns `YEAR_VAL` and `STUDENT_STD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "Name: YEAR_VAL, dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which rows have a value in YEAR_VAL that is higher then the YEAR_MEAN\n",
    "sc.vv_nn.YEAR_VAL.apply(lambda x : x > sc.vv_nn.YEAR_VAL.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "Name: STUDENT_STD, dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which rows have a value in STUDENT_STD that is higher then the STUDENT_STD mean\n",
    "sc.vv_nn.STUDENT_STD.apply(lambda x : x > sc.vv_nn.STUDENT_STD.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Complexity of Express and Zeit <a class=\"anchor\" id=\"expresszeit\"></a>\n",
    "\n",
    "In this section, the syntactic complexity features will be calculated on texts from our *Express* and *Zeit* corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get the filenames of the `.conllup` files with the annoation for the reference corpora (136 files from *Express* and 137 files from *Zeit*) and save them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import function listdir from module os\n",
    "from os import listdir\n",
    "\n",
    "# define path to the files\n",
    "#ez_path = \"data/corpus/random_BIO/\"\n",
    "ez_path = \"data/\"\n",
    "\n",
    "# save filenames in list\n",
    "#ez_filenames = listdir(ez_path)\n",
    "\n",
    "# demo data\n",
    "ez_filenames = ['express_1.conllup', 'express_2.conllup', 'express_3.conllup', \n",
    "                'zeit_1.conllup', 'zeit_2.conllup', 'zeit_3.conllup'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define a function that takes the list of filenames and makes a dictionary with key=`(corpus ID, text number)` and value=`DataFrame`. \n",
    "\n",
    "It is similar to the function `make_df_dict` from the module `functions`, but specific to the corpora's texts' annotation that slightly differs from that of the Abitur texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corp_dict(path: str, filenames: list):\n",
    "    \"\"\"\n",
    "    Function that takes a path and filenames and returns a dictionary with key = (corpus ID, text number) and value = data frame.\n",
    "    The corpus ID for Express is 1 and the corpus ID for Zeit is 2. \"\"\"\n",
    "\n",
    "    df_dict = dict()\n",
    "\n",
    "    for i, filename in enumerate(filenames):\n",
    "\n",
    "        # open connlup file and use first line for saving the column names for data frame\n",
    "        with open(path+filename, \"r\", encoding=\"UTF-8\") as file:\n",
    "            column_names = file.readline().replace(\"# global.columns =\", \"\").strip().split()\n",
    "        \n",
    "        # load file into data frame\n",
    "        df = pd.read_csv(path+filename, comment=\"#\", sep=\"\\t\", quoting=3, header=None, names=column_names)\n",
    "\n",
    "        # convert FORM column values into strings\n",
    "        df[\"FORM\"] = df[\"FORM\"].astype(str)\n",
    "        # delete rows with no words\n",
    "        df = df[~df.FORM.str.contains(\"EMPTY\")]\n",
    "        # delete superfluous rows for only one word\n",
    "        df = df[~df.FORM.str.contains(r\"<[IE]->\")]\n",
    "        # reset index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df[\"TREE\"] = df[\"TREE\"].astype(str)\n",
    "        # make new column SYNTAX that is like the column TREE but without the PSEUDO tags\n",
    "        df[\"SYNTAX\"] = df.TREE.str.replace(r\"[BIE]-PSEUDO\\|?\", '', regex=True)\n",
    "        # ignore rows that have no (meaningful) syntax annotation\n",
    "        df = df[df.SYNTAX.apply(len) > 1]\n",
    "        # reset index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # make sentence ID\n",
    "        df.loc[[0],[\"ID\"]] = 1\n",
    "        df[\"SENT_ID\"] = df.ID.eq(1).cumsum() - 1\n",
    "\n",
    "        # only keep columns that are needed\n",
    "        keep = [\"FORM\", \"SENT_ID\", \"SYNTAX\", \"XPOS\"]\n",
    "        df = df[keep]\n",
    "        \n",
    "        # instead of year numbers, use corpora IDs for the first part of the key tuple\n",
    "        if filename.startswith(\"express\"):\n",
    "            df_dict[(1, i+1)] = df\n",
    "        if filename.startswith(\"zeit\"):\n",
    "            df_dict[(2, i+1)] = df\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function\n",
    "corp_dict = make_corp_dict(ez_path, ez_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(1, 1), (1, 2), (1, 3), (2, 4), (2, 5), (2, 6)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to look at example dataframe from the dict\n",
    "#corp_dict[(1, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the class `SynComplMeas` to calculate each syntactic complexity measure on the Zeit and Express corpora's texts. \n",
    "\n",
    "The result data frames will only have two rows: One for the Express corpus and one for the Zeit corpus. \n",
    "\n",
    "Here, `YEAR` always stands for the corpus ID (Express=`1`, Zeit=`2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load class\n",
    "sc2 = SynComplMeas(name=\"Express and Zeit Syntactic Complexity\", df_dict=corp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>YEAR_VAL</th>\n",
       "      <th>STUDENT_VALS</th>\n",
       "      <th>STUDENT_STD</th>\n",
       "      <th>YEARS_MEAN</th>\n",
       "      <th>YEARS_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.548979</td>\n",
       "      <td>[0.47706422018348627, 0.6282051282051282, 0.54...</td>\n",
       "      <td>0.075835</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>0.099414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.408385</td>\n",
       "      <td>[0.36607142857142855, 0.3879310344827586, 0.47...</td>\n",
       "      <td>0.055447</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>0.099414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  YEAR_VAL                                       STUDENT_VALS  \\\n",
       "0     1  0.548979  [0.47706422018348627, 0.6282051282051282, 0.54...   \n",
       "1     2  0.408385  [0.36607142857142855, 0.3879310344827586, 0.47...   \n",
       "\n",
       "   STUDENT_STD  YEARS_MEAN  YEARS_STD  \n",
       "0     0.075835    0.478682   0.099414  \n",
       "1     0.055447    0.478682   0.099414  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show dataframe for one attribute\n",
    "sc2.vv_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one result data frame is very short, it makes sense to concatenate all data frames into one big dataframe and add a column that displays the respective feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty data frame\n",
    "ez_df = pd.DataFrame()\n",
    "\n",
    "# save result dfs in list\n",
    "ez_results = [sc2.sent_lens, sc2.tok_embeds, sc2.max_sent_embeds, sc2.simpx_s, sc2.subc_s, sc2.relc_s, sc2.parac_s, sc2.clauses_s, sc2.verbx_s, sc2.vc_s, sc2.nx_s, sc2.simpx_c,\n",
    "            sc2.subc_c, sc2.relc_c, sc2.parac_c, sc2.clause_lens, sc2.simpx_lens, sc2.relc_lens, sc2.nx_lens, sc2.px_lens, sc2.vf_lens, sc2.mf_lens, sc2.nf_lens, sc2.vv_nn]\n",
    "\n",
    "# iterate over data frames and feature names\n",
    "for df, df_name in zip(ez_results, sc2.feature_names):\n",
    "    # create column for feature name\n",
    "    df[\"FEAT\"] = df_name\n",
    "    # concatenate dfs\n",
    "    ez_df = pd.concat([ez_df, df])\n",
    "\n",
    "# change column order\n",
    "ez_df = ez_df[[\"FEAT\", \"YEAR\", \"YEAR_VAL\", \"YEARS_MEAN\", \"YEARS_STD\"]]\n",
    "ez_df[\"YEAR\"].replace({1: \"E\", 2: \"Z\"}, inplace=True)\n",
    "ez_df.rename({\"YEAR\": \"COR\", \"YEAR_VAL\": \"VAL\", \"YEARS_MEAN\": \"MEAN\"}, axis='columns', inplace=True)\n",
    "\n",
    "# save data frame as .csv file\n",
    "#ez_df.to_csv(\"results/3_syntax/expr_zeit/express_zeit.csv\")\n",
    "ez_df.to_csv(\"results/3_syntax_demo/express_zeit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>COR</th>\n",
       "      <th>VAL</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>YEARS_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sent_lens</td>\n",
       "      <td>E</td>\n",
       "      <td>14.950202</td>\n",
       "      <td>19.099738</td>\n",
       "      <td>5.868330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sent_lens</td>\n",
       "      <td>Z</td>\n",
       "      <td>23.249274</td>\n",
       "      <td>19.099738</td>\n",
       "      <td>5.868330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tok_embeds</td>\n",
       "      <td>E</td>\n",
       "      <td>3.020896</td>\n",
       "      <td>3.249037</td>\n",
       "      <td>0.322641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tok_embeds</td>\n",
       "      <td>Z</td>\n",
       "      <td>3.477179</td>\n",
       "      <td>3.249037</td>\n",
       "      <td>0.322641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max_sent_embeds</td>\n",
       "      <td>E</td>\n",
       "      <td>4.324755</td>\n",
       "      <td>4.849601</td>\n",
       "      <td>0.742245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max_sent_embeds</td>\n",
       "      <td>Z</td>\n",
       "      <td>5.374447</td>\n",
       "      <td>4.849601</td>\n",
       "      <td>0.742245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simpx_s</td>\n",
       "      <td>E</td>\n",
       "      <td>1.614616</td>\n",
       "      <td>1.731427</td>\n",
       "      <td>0.165196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simpx_s</td>\n",
       "      <td>Z</td>\n",
       "      <td>1.848238</td>\n",
       "      <td>1.731427</td>\n",
       "      <td>0.165196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subc_s</td>\n",
       "      <td>E</td>\n",
       "      <td>0.441750</td>\n",
       "      <td>0.547987</td>\n",
       "      <td>0.150243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subc_s</td>\n",
       "      <td>Z</td>\n",
       "      <td>0.654225</td>\n",
       "      <td>0.547987</td>\n",
       "      <td>0.150243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relc_s</td>\n",
       "      <td>E</td>\n",
       "      <td>0.168572</td>\n",
       "      <td>0.253952</td>\n",
       "      <td>0.120746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relc_s</td>\n",
       "      <td>Z</td>\n",
       "      <td>0.339333</td>\n",
       "      <td>0.253952</td>\n",
       "      <td>0.120746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clauses_s</td>\n",
       "      <td>E</td>\n",
       "      <td>1.783188</td>\n",
       "      <td>1.997284</td>\n",
       "      <td>0.302778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clauses_s</td>\n",
       "      <td>Z</td>\n",
       "      <td>2.211381</td>\n",
       "      <td>1.997284</td>\n",
       "      <td>0.302778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>verbx_s</td>\n",
       "      <td>E</td>\n",
       "      <td>2.562324</td>\n",
       "      <td>2.661609</td>\n",
       "      <td>0.140411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>verbx_s</td>\n",
       "      <td>Z</td>\n",
       "      <td>2.760894</td>\n",
       "      <td>2.661609</td>\n",
       "      <td>0.140411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vc_s</td>\n",
       "      <td>E</td>\n",
       "      <td>1.079670</td>\n",
       "      <td>1.189671</td>\n",
       "      <td>0.155564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vc_s</td>\n",
       "      <td>Z</td>\n",
       "      <td>1.299672</td>\n",
       "      <td>1.189671</td>\n",
       "      <td>0.155564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nx_s</td>\n",
       "      <td>E</td>\n",
       "      <td>5.731738</td>\n",
       "      <td>7.596196</td>\n",
       "      <td>2.636742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nx_s</td>\n",
       "      <td>Z</td>\n",
       "      <td>9.460654</td>\n",
       "      <td>7.596196</td>\n",
       "      <td>2.636742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clause_lens</td>\n",
       "      <td>E</td>\n",
       "      <td>11.789953</td>\n",
       "      <td>13.774798</td>\n",
       "      <td>2.806995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clause_lens</td>\n",
       "      <td>Z</td>\n",
       "      <td>15.759643</td>\n",
       "      <td>13.774798</td>\n",
       "      <td>2.806995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simpx_lens</td>\n",
       "      <td>E</td>\n",
       "      <td>12.231237</td>\n",
       "      <td>14.061898</td>\n",
       "      <td>2.588945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simpx_lens</td>\n",
       "      <td>Z</td>\n",
       "      <td>15.892558</td>\n",
       "      <td>14.061898</td>\n",
       "      <td>2.588945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relc_lens</td>\n",
       "      <td>E</td>\n",
       "      <td>7.658333</td>\n",
       "      <td>10.391071</td>\n",
       "      <td>3.864675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relc_lens</td>\n",
       "      <td>Z</td>\n",
       "      <td>13.123810</td>\n",
       "      <td>10.391071</td>\n",
       "      <td>3.864675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nx_lens</td>\n",
       "      <td>E</td>\n",
       "      <td>2.121715</td>\n",
       "      <td>2.503370</td>\n",
       "      <td>0.539741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nx_lens</td>\n",
       "      <td>Z</td>\n",
       "      <td>2.885025</td>\n",
       "      <td>2.503370</td>\n",
       "      <td>0.539741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>px_lens</td>\n",
       "      <td>E</td>\n",
       "      <td>3.402316</td>\n",
       "      <td>3.948182</td>\n",
       "      <td>0.771971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>px_lens</td>\n",
       "      <td>Z</td>\n",
       "      <td>4.494048</td>\n",
       "      <td>3.948182</td>\n",
       "      <td>0.771971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vf_lens</td>\n",
       "      <td>E</td>\n",
       "      <td>2.272398</td>\n",
       "      <td>2.914915</td>\n",
       "      <td>0.908656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vf_lens</td>\n",
       "      <td>Z</td>\n",
       "      <td>3.557431</td>\n",
       "      <td>2.914915</td>\n",
       "      <td>0.908656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mf_lens</td>\n",
       "      <td>E</td>\n",
       "      <td>4.669180</td>\n",
       "      <td>5.180343</td>\n",
       "      <td>0.722893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mf_lens</td>\n",
       "      <td>Z</td>\n",
       "      <td>5.691505</td>\n",
       "      <td>5.180343</td>\n",
       "      <td>0.722893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nf_lens</td>\n",
       "      <td>E</td>\n",
       "      <td>8.789394</td>\n",
       "      <td>10.154221</td>\n",
       "      <td>1.930157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nf_lens</td>\n",
       "      <td>Z</td>\n",
       "      <td>11.519048</td>\n",
       "      <td>10.154221</td>\n",
       "      <td>1.930157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              FEAT COR        VAL       MEAN  YEARS_STD\n",
       "0        sent_lens   E  14.950202  19.099738   5.868330\n",
       "1        sent_lens   Z  23.249274  19.099738   5.868330\n",
       "0       tok_embeds   E   3.020896   3.249037   0.322641\n",
       "1       tok_embeds   Z   3.477179   3.249037   0.322641\n",
       "0  max_sent_embeds   E   4.324755   4.849601   0.742245\n",
       "1  max_sent_embeds   Z   5.374447   4.849601   0.742245\n",
       "0          simpx_s   E   1.614616   1.731427   0.165196\n",
       "1          simpx_s   Z   1.848238   1.731427   0.165196\n",
       "0           subc_s   E   0.441750   0.547987   0.150243\n",
       "1           subc_s   Z   0.654225   0.547987   0.150243\n",
       "0           relc_s   E   0.168572   0.253952   0.120746\n",
       "1           relc_s   Z   0.339333   0.253952   0.120746\n",
       "0        clauses_s   E   1.783188   1.997284   0.302778\n",
       "1        clauses_s   Z   2.211381   1.997284   0.302778\n",
       "0          verbx_s   E   2.562324   2.661609   0.140411\n",
       "1          verbx_s   Z   2.760894   2.661609   0.140411\n",
       "0             vc_s   E   1.079670   1.189671   0.155564\n",
       "1             vc_s   Z   1.299672   1.189671   0.155564\n",
       "0             nx_s   E   5.731738   7.596196   2.636742\n",
       "1             nx_s   Z   9.460654   7.596196   2.636742\n",
       "0      clause_lens   E  11.789953  13.774798   2.806995\n",
       "1      clause_lens   Z  15.759643  13.774798   2.806995\n",
       "0       simpx_lens   E  12.231237  14.061898   2.588945\n",
       "1       simpx_lens   Z  15.892558  14.061898   2.588945\n",
       "0        relc_lens   E   7.658333  10.391071   3.864675\n",
       "1        relc_lens   Z  13.123810  10.391071   3.864675\n",
       "0          nx_lens   E   2.121715   2.503370   0.539741\n",
       "1          nx_lens   Z   2.885025   2.503370   0.539741\n",
       "0          px_lens   E   3.402316   3.948182   0.771971\n",
       "1          px_lens   Z   4.494048   3.948182   0.771971\n",
       "0          vf_lens   E   2.272398   2.914915   0.908656\n",
       "1          vf_lens   Z   3.557431   2.914915   0.908656\n",
       "0          mf_lens   E   4.669180   5.180343   0.722893\n",
       "1          mf_lens   Z   5.691505   5.180343   0.722893\n",
       "0          nf_lens   E   8.789394  10.154221   1.930157\n",
       "1          nf_lens   Z  11.519048  10.154221   1.930157"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display data frame with relevant features in notebook (only rows with YEARS_STD > 0.1)\n",
    "ez_df[ez_df.YEARS_STD > 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Two Example Sentences\n",
    "\n",
    "(Note: this does not work with the demo data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(2003, 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3046233c6e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mex_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (2003, 18)"
     ]
    }
   ],
   "source": [
    "ex_df = df_dict[(2003, 18)]\n",
    "print(ex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['WEBANNO', 'FORM'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3454d0014337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mshort_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSENT_ID\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m83\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshort_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SENT_ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FORM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"XPOS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SYNTAX\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WEBANNO\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['WEBANNO', 'FORM'] not in index\""
     ]
    }
   ],
   "source": [
    "short_sent = ex_df[ex_df.SENT_ID==83]\n",
    "short_sent[[\"SENT_ID\", \"FORM\", \"XPOS\", \"SYNTAX\", \"WEBANNO\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['WEBANNO', 'FORM'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-19c068f84272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlong_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSENT_ID\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlong_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SENT_ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FORM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"XPOS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SYNTAX\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WEBANNO\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['WEBANNO', 'FORM'] not in index\""
     ]
    }
   ],
   "source": [
    "long_sent = ex_df[ex_df.SENT_ID==5]\n",
    "long_sent[[\"SENT_ID\", \"FORM\", \"XPOS\", \"SYNTAX\", \"WEBANNO\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Length: 0\n",
      "Clauses in Sentence: 0\n",
      "Subordinate Clauses in Sentence: 0\n",
      "Mean Clause Length: nan\n",
      "Mean Simplex Clause Length: nan\n",
      "Mean Relative Clause Length: nan\n",
      "Simplex Clauses in Sentence: 0\n",
      "Relative Clauses in Sentence: 0\n",
      "Paratactic Clauses in Sentence: 0\n",
      "Mean Prefield Length: nan\n",
      "Mean Middle Field Length: nan\n",
      "Mean Postfield Length: nan\n",
      "Mean NP Length: nan\n",
      "Mean PP Length: nan\n",
      "Verbs in Sentence: 0\n",
      "NPs in Sentence: 0\n",
      "Verb/Noun Ratio: nan\n",
      "Mean Token Embedding Depth: nan\n",
      "Maximum Embedding Depth: nan\n",
      "\n",
      "\n",
      "Sentence Length: 10\n",
      "Clauses in Sentence: 1\n",
      "Subordinate Clauses in Sentence: 0\n",
      "Mean Clause Length: 9.0\n",
      "Mean Simplex Clause Length: 9.0\n",
      "Mean Relative Clause Length: nan\n",
      "Simplex Clauses in Sentence: 1\n",
      "Relative Clauses in Sentence: 0\n",
      "Paratactic Clauses in Sentence: 0\n",
      "Mean Prefield Length: 3.0\n",
      "Mean Middle Field Length: 5.0\n",
      "Mean Postfield Length: nan\n",
      "Mean NP Length: 2.6666666666666665\n",
      "Mean PP Length: nan\n",
      "Verbs in Sentence: 1\n",
      "NPs in Sentence: 3\n",
      "Verb/Noun Ratio: 0.5\n",
      "Mean Token Embedding Depth: 2.9\n",
      "Maximum Embedding Depth: 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/calc_syn_complexity.py:431: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  phrase_len = phrase_tag_count/phrase_count\n",
      "/home/dipper/.local/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/dipper/.local/lib/python3.6/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "top_fields_RE = r\"\"\"(?x)    # flag verbose\n",
    "                        \\|          # beginning hyphen\n",
    "                        (\n",
    "                        ([BIE]-)?   # optional: B- or I- or E-\n",
    "                         (\n",
    "                         [VMN]FE?   # VF or MF or NF or MFE\n",
    "                         |\n",
    "                         L[KV]      # LK or LV\n",
    "                         |\n",
    "                         F?KOORD    # FKOORD or KOORD\n",
    "                         |\n",
    "                         PARORD\n",
    "                         |\n",
    "                         V?CE?      # VC or VCE or C\n",
    "                         |\n",
    "                         FKONJ\n",
    "                         )\n",
    "                        )\n",
    "                        (?= $|\\|)   # lookahead: should be there but won't be replaced\n",
    "                        \"\"\"\n",
    "\n",
    "ex_sents = [short_sent, long_sent]\n",
    "\n",
    "for sent in ex_sents:\n",
    "    #print(\"Sentence:\", \" \".join(sent.FORM))\n",
    "    print()\n",
    "    print(\"Sentence Length:\", len(sent))\n",
    "    print(\"Clauses in Sentence:\", sc.count_pattern(sent, r\"(^|\\|)(B-)?[PR]?-?SIMPX\"))\n",
    "    print(\"Subordinate Clauses in Sentence:\", sc.count_pattern(sent, r\"(^|\\|)(B-)?C($|\\|)\"))\n",
    "    print(\"Mean Clause Length:\", sc.get_phrase_lens(sent, r\"[PR]?-?SIMPX\"))\n",
    "    print(\"Mean Simplex Clause Length:\", sc.get_phrase_lens(sent, r\"SIMPX\"))\n",
    "    print(\"Mean Relative Clause Length:\", sc.get_phrase_lens(sent, r\"R-?SIMPX\"))\n",
    "    print(\"Simplex Clauses in Sentence:\", sc.count_pattern(sent, r\"(^|\\|)(B-)?SIMPX\"))\n",
    "    print(\"Relative Clauses in Sentence:\", sc.count_pattern(sent, r\"(^|\\|)(B-)?R-?SIMPX\"))\n",
    "    print(\"Paratactic Clauses in Sentence:\", sc.count_pattern(sent, r\"(^|\\|)(B-)?P-?SIMPX\"))\n",
    "    print(\"Mean Prefield Length:\", sc.get_phrase_lens(sent, r\"VF\"))\n",
    "    print(\"Mean Middle Field Length:\", sc.get_phrase_lens(sent, r\"MF\"))\n",
    "    print(\"Mean Postfield Length:\", sc.get_phrase_lens(sent, r\"NF\"))\n",
    "    print(\"Mean NP Length:\", sc.get_phrase_lens(sent, r\"NX\"))\n",
    "    print(\"Mean PP Length:\", sc.get_phrase_lens(sent, r\"PX\"))\n",
    "    print(\"Verbs in Sentence:\", sc.count_pattern(sent, r\"(^|\\|)(B-)?VXF?INF?\"))\n",
    "    print(\"NPs in Sentence:\", sc.count_pattern(sent, r\"(^|\\|)(B-)?NX\"))\n",
    "    print(\"Verb/Noun Ratio:\", sc.count_pattern(sent, r\"VV.*\")/sc.count_pattern(sent, r\"NN\"))\n",
    "    print(\"Mean Token Embedding Depth:\", sc.get_tok_embeds(sent, top_fields_RE))\n",
    "    sent[\"TEMP\"] = sent.SYNTAX.str.replace(top_fields_RE, '', regex=True).str.split(\"|\").apply(len)\n",
    "    print(\"Maximum Embedding Depth:\", sent.TEMP.max())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a class=\"anchor\" id=\"ref\"></a>\n",
    "\n",
    "Kristian Berg, Jonas Romstadt, and Cedrek Neitzert. 2021. GraphVar â€“ Korpusaufbau und Annotation. Version 1.0. Friedrich-Wilhelms-UniversitÃ¤t Bonn, https://graphvar.uni-bonn.de/dokumentation.html.\n",
    "\n",
    "Miao Chen and Klaus Zechner. 2011. Computing and evaluating syntactic complexity features for auto- mated scoring of spontaneous non-native speech. In *Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies*, pages 722â€“731, Portland, Oregon, USA. Association for Computational Linguistics\n",
    "\n",
    "Jennifer Meyer, Torben Jansen, Johanna Fleckenstein, Stefan Keller, and Olaf KÃ¶ller. 2020. Machine Learning im Bildungskonstext: Evidenz fÃ¼r die Genauigkeit der automatisierten Beurteilung von Essays im Fach Englisch. *Zeitschrift fÃ¼r PÃ¤dagogische Psychologie*, 0:1â€“12."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20ec976ec051167535b3472cc1ed33475659b92bfb7468fe2abcc444e793768c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
